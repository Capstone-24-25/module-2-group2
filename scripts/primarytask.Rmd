---
title: "primary task"
output: pdf_document
date: "2024-11-18"
---
```{r}
library(rvest)
library(tidyverse)
require(tidytext)
require(textstem)
require(rvest)
require(qdapRegex)
require(stopwords)
require(tokenizers)
library(tidymodels)
library(modelr)
library(Matrix)
library(sparsesvd)
library(glmnet)
library(e1071) # SVM library
library(yardstick)
```


```{r}
# Loading the Data
source('preprocessing.R')
load("../data/claims-raw.RData")
load("../data/claims-clean-example.RData")
load('../data/claims-clean-headers.RData')
```

### Tokenize into Unigrams
```{r}
# Set seed for reproducibility
set.seed(1234)
# Tokenize into unigrams
headers_unigrams <- claims_clean_headers %>% 
  select(.id, bclass, mclass, text_clean) %>% 
  unnest_tokens(output = word, 
                input = text_clean, 
                token = 'words', 
                stopwords = str_remove_all(stop_words$word,'[[:punct:]]'))

```

### Change Unigram Data into a TF-IDF
```{r}
# Count unigrams and compute TF-IDF
headers_unigrams_tfidf <- headers_unigrams %>% 
  count(.id, bclass, mclass, word, name = 'n') %>% 
  bind_tf_idf(term = word,
              document = .id, 
              n = n) %>% 
  filter(n>=5) %>%
  pivot_wider(id_cols = c(.id, bclass, mclass), 
              names_from = word,
              values_from = tf_idf,
              values_fill = 0)
```

### Partition the Data
```{r}
set.seed(12345)
# Partition data
partitions_unigrams <- headers_unigrams_tfidf %>% initial_split(prop = 0.8)

train_dtm_unigrams <- training(partitions_unigrams) %>% 
  select(-.id, -bclass, -mclass)
train_labels_unigrams <- training(partitions_unigrams) %>% 
  select(.id, bclass, mclass)

test_dtm_unigrams <- testing(partitions_unigrams) %>% 
  select(-.id, -bclass, -mclass)
test_labels_unigrams <- testing(partitions_unigrams) %>% 
  select(.id, bclass, mclass)
```

### Dimensionality Reduction of training and testing splits
```{r}
# PCA projection for training unigram data
train_dtm_unigrams_sparse <- train_dtm_unigrams %>% 
  as.matrix() %>%
  as('sparseMatrix')
svd_out_unigrams <- sparsesvd(train_dtm_unigrams_sparse, rank=173) 

# Training PCs data frame
train_dtm_projected2 <- svd_out_unigrams$u %*% diag(svd_out_unigrams$d)


# Assign column names
colnames(train_dtm_projected2) <- paste0("PC", 1:ncol(train_dtm_projected2))
```

```{r}
# Function to reproject test data into the same space as training data
reproject_fn1 <- function(.dtm, svd_out) {
  # Convert test data into a sparse matrix
  .dtm_sparse <- as(.dtm, "sparseMatrix")
  
  # Reproject test data into the same space using SVD's V and D components
  test_projected <- as.matrix(.dtm_sparse %*% svd_out$v %*% diag(1 / svd_out$d))
  
  # Assign column names for principal components
  colnames(test_projected) <- paste0("PC", 1:ncol(test_projected))
  
  return(test_projected)
}

# Apply function to project test data
test_dtm_projected2 <- reproject_fn1(.dtm = test_dtm_unigrams, svd_out = svd_out_unigrams)
```

Test data is now represented in the same PCA-transformed space as the training data.

### Logistic Regression for Binary Model

```{r}
train <- train_labels_unigrams %>% 
  transmute(bclass = factor(bclass)) %>% 
  bind_cols(train_dtm_projected2)

fit <- glm(bclass ~ ., data = train, family = binomial)
```

```{r}
# store predictors and response as matrix and vector
x_train <- train %>% select(-bclass) %>%  as.matrix()
y_train <- train_labels_unigrams %>% pull(bclass)

# fit enet model
alpha_enet <- 0.3
log_reg_binary <- glmnet(x = x_train,
                  y = y_train,
                  family = 'binomial', 
                  alpha = alpha_enet)

# choose a constraint strength by cross-validation
set.seed(12345)
cvout <- cv.glmnet(x = x_train,
                   y = y_train,
                   family = 'binomial',
                   alpha = alpha_enet)

# store optimal strength
lambda_opt <- cvout$lambda.min

cvout
```

### Predictions for Binary Model
```{r}
# coerce to matrix
x_test <- as.matrix(test_dtm_projected2)

# compute predicted probabilities
preds <- predict(log_reg_binary,
                 s = lambda_opt,
                 newx = x_test,
                 type = 'response')

```

Now we can bind the test labels to the predictions:

```{r}
# store predictions in a data frame with true labels 
pred_df <- test_labels_unigrams %>% 
  transmute(bclass = factor(bclass)) %>% 
  bind_cols(pred = as.numeric(preds)) %>% 
  mutate(bclass.pred = factor(pred > 0.5,
                              labels = levels(bclass)))

# define classification metric panel
panel <- metric_set(sensitivity,
                    specificity,
                    accuracy,
                    roc_auc)

# compute test set accuracy
pred_df %>%  panel(truth = bclass,
                   estimate = bclass.pred,
                   pred,
                   event_level = 'second')
  
```

## Multinomial regression for Multiclass Model
```{r}
# get multiclass labels 
y_train_multi <- train_labels_unigrams %>% pull(mclass)

# fit enet mdoel
alpha_enet <- 0.2
log_reg_multi <- glmnet(x = x_train,
                        y = y_train_multi,
                        family = 'multinomial',
                        alpha = alpha_enet)

# choose strenth by cross-validation
set.seed(12345)
cvout_multi <- cv.glmnet(x = x_train,
                         y = y_train_multi,
                         family = 'multinomial',
                         alpha = alpha_enet)

cvout_multi
```

# Predictions for Multi-class Model

```{r}
preds_multi <- predict(log_reg_multi,
                       s = cvout_multi$lambda.min,
                       newx = x_test,
                       type = 'response')
as_tibble(preds_multi[,,1]) %>% head(5)
```

Now we choose the most probable class as the prediction and cross-tabulate with the actual label.

```{r}
pred_class <- as_tibble(preds_multi[,,1]) %>% 
  mutate(row = row_number()) %>% 
  pivot_longer(-row,
               names_to = 'label',
               values_to = 'probability') %>% 
  group_by(row) %>% 
  slice_max(probability, n = 1) %>% 
  pull(label)

pred_tbl <- table(pull(test_labels_unigrams, mclass), pred_class)

pred_tbl
```

### Save the models
```{r}
# Save the logistic regression binary model
save(log_reg_binary, file = "../data/log_reg_binary.RData")

# Save the multinomial regression multi-class model
save(log_reg_multi, file = "../data/log_reg_multi.RData")
```

# Predition on claims-test.RData
```{r}
# Load in the new test data
load("../data/claims-test.RData")
```

## Clean the test data
```{r}
# Clean test data
#claims_test_clean <- claims_test %>%
  #parse_data()

claims_test_clean %>% head(5)
```

## Tokenize
```{r}
# Set seed for reproducibility
set.seed(1234)
# Tokenize into unigrams
test_unigrams <- claims_test_clean %>% 
  select(.id, text_clean) %>% 
  unnest_tokens(output = word, 
                input = text_clean, 
                token = 'words', 
                stopwords = str_remove_all(stop_words$word,'[[:punct:]]'))
```

## Convert to TF-IDF
```{r}
# Count unigrams and compute TF-IDF
test_unigrams_tfidf <- test_unigrams %>% 
  count(.id, word, name = 'n') %>% 
  bind_tf_idf(term = word,
              document = .id, 
              n = n) %>% 
  filter(n>=5) %>%
  pivot_wider(id_cols = c(.id), 
              names_from = word,
              values_from = tf_idf,
              values_fill = 0)
```

### Dimensionality Reduction of Test Data
```{r}
test_unigrams_tfidf_mtx <- test_unigrams_tfidf %>% select(-.id)

# PCA projection for training unigram data
test_dtm_unigrams_sparse <- test_unigrams_tfidf_mtx %>% 
  as.matrix() %>%
  as('sparseMatrix')


svd_out_unigrams <- sparsesvd(test_dtm_unigrams_sparse, rank=173)

# project test data onto PCs
test_dtm_projected<- reproject_fn1(.dtm = test_unigrams_tfidf_mtx, svd_out = svd_out_unigrams)
```

### Make out predictions for both models
```{r}
## For binary model
# coerce to matrix
x_test_actual <- as.matrix(test_dtm_projected)

# compute predicted probabilities
preds_actual <- predict(log_reg_binary,
                 s = lambda_opt,
                 newx = x_test_actual,
                 type = 'response')

# get the labels
pred_df_binary <-  bind_cols(.id = test_unigrams_tfidf$.id, pred = as.numeric(preds_actual)) %>% 
  mutate(bclass.pred = factor(pred > 0.5, 
                              labels = levels(test_labels_unigrams$bclass))) %>% 
  select(.id, bclass.pred)

## For multi-class model
preds_multi_actual <- predict(log_reg_multi,
                       s = cvout_multi$lambda.min,
                       newx = x_test_actual,
                       type = 'response')

# get the labels for multi classes
pred_class_actual <- as_tibble(preds_multi_actual[,,1]) %>% 
  mutate(row = row_number()) %>% 
  pivot_longer(-row,
               names_to = 'label',
               values_to = 'probability') %>% 
  group_by(row) %>% 
  slice_max(probability, n = 1) %>% 
  pull(label)

pred_df_multi <- tibble(.id = test_unigrams_tfidf$.id,
                        mclass.pred = factor(pred_class_actual,
                                             levels = colnames(preds_multi_actual)))

# Combine binary and multi-class predictions
pred_df <- left_join(pred_df_binary, pred_df_multi, by = ".id")

# Save predictions
save(pred_df, file = "../data/pred_df.RData")
```











