---
title: "preliminary taks"
output: html_document
date: "2024-11-14"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Functions and libraries

```{r}
library(tidyverse)
require(tidytext)
require(textstem)
require(rvest)
require(qdapRegex)
require(stopwords)
require(tokenizers)
library(tidymodels)
library(modelr)
library(Matrix)
library(sparsesvd)
library(glmnet)
# function to parse html and clean text
parse_fn <- function(.html){
  read_html(.html) %>%
    html_elements('p, h1, h2, h3 , h4, h5, h6') %>%
    html_text2() %>%
    str_c(collapse = ' ') %>%
    rm_url() %>%
    rm_email() %>%
    str_remove_all('\'') %>%
    str_replace_all(paste(c('\n', 
                            '[[:punct:]]', 
                            'nbsp', 
                            '[[:digit:]]', 
                            '[[:symbol:]]'),
                          collapse = '|'), ' ') %>%
    str_replace_all("([a-z])([A-Z])", "\\1 \\2") %>%
    tolower() %>%
    str_replace_all("\\s+", " ")
}

# function to apply to claims data
parse_data <- function(.df){
  out <- .df %>%
    filter(str_detect(text_tmp, '<!')) %>%
    rowwise() %>%
    mutate(text_clean = parse_fn(text_tmp)) %>%
    unnest(text_clean) 
  return(out)
}

#PCA project function
# function
projection_fn <- function(.dtm, .prop){
  # coerce feature matrix to sparse
  dtm_mx <- .dtm %>%
    as.matrix() %>%
    as('sparseMatrix')
  
  # compute svd
  svd_out <- sparsesvd(dtm_mx)
  
  # select number of projections
  var_df <- tibble(var = svd_out$d^2) %>%
    mutate(pc = row_number(),
           cumulative = cumsum(var)/sum(var))
  
  n_pc <- which.min(var_df$cumulative < .prop)
  
  # extract loadings
  loadings <- svd_out$v[, 1:n_pc] %>% as.matrix()
  
  # extract scores
  scores <- (dtm_mx %*% svd_out$v[, 1:n_pc]) %>% as.matrix()
  
  # adjust names
  colnames(loadings) <- colnames(scores) <- paste('pc', 1:n_pc, sep = '')
  
  # output
    out <- list(n_pc = n_pc,
              var = var_df,
              projection = loadings,
              data = as_tibble(scores))
  
  return(out)
}

```

# Task 1

add headers to data

```{r}
claims_clean_headers <- claims_raw %>%
  parse_data()
```

change data with headers into tf-idf

```{r}
headers_clean <- claims_clean_headers %>%
  filter(str_detect(text_tmp, '<!')) %>%
  rowwise() %>%
  mutate(text_clean = parse_fn(text_tmp)) %>%
  select(-text_tmp) %>%
  unnest(text_clean) %>% 
  select(-c(1:4), -6)

headers_tfidf <- headers_clean %>% 
  unnest_tokens(output = token, 
                input = text_clean, 
                token = 'words',
                stopwords = str_remove_all(stop_words$word, 
                                           '[[:punct:]]')) %>%
  mutate(token.lem = lemmatize_words(token)) %>%
  filter(str_length(token.lem) > 2) %>%
  count(.id, bclass, mclass, token.lem, name = 'n') %>%
  bind_tf_idf(term = token.lem, 
              document = .id,
              n = n) %>%
  pivot_wider(id_cols = c('.id', 'bclass', 'mclass'),
              names_from = 'token.lem',
              values_from = 'tf_idf',
              values_fill = 0)

headers_tfidf %>% head(3)

```

partition data

```{r}
# Step 1: Partitioning
# partition data
set.seed(102722)
partitions <- headers_tfidf %>% initial_split(prop = 0.8)

# separate DTM from labels
test_dtm <- testing(partitions) %>%
  select(-.id, -bclass, -mclass)
test_labels <- testing(partitions) %>%
  select(.id, bclass, mclass)

# same, training set
train_dtm <- training(partitions) %>%
  select(-.id, -bclass, -mclass)
train_labels <- training(partitions) %>%
  select(.id, bclass, mclass)
```

PCA with header data

```{r}
# Step 2: Projection
# find projections based on training data
proj_out <- projection_fn(.dtm = train_dtm, .prop = 0.7)
train_dtm_projected <- proj_out$data

# how many components were used?
proj_out$n_pc
```

fit header data into logistic regressiong

```{r}
# Step 3: Regression
train <- train_labels %>%
  transmute(bclass = factor(bclass)) %>%
  bind_cols(train_dtm_projected)


fit <- glm(bclass~., data = train, family = binomial)
summary(fit)
```

```{r}
# Evaluate errors on test set
  class_metrics <- metric_set(sensitivity, 
                              specificity, 
                              accuracy,
                              roc_auc)
  
  # Calculate metrics and create kable for each protein vector
  metrics_result <- test_labels %>%
    add_predictions(fit, type = 'response') %>%
    mutate(est = as.factor(pred > 0.5), tr_c = as.factor(class)) %>%
    class_metrics(estimate = est,
                  truth = tr_c, pred,
                  event_level = 'second')
```

